Steps Done: 
Dataset download link: https://github.com/zeyongj/House-Prices-Advanced-Regression-Techniques/blob/main/train.csv

Loading the dataset in googe colab:
import pandas as pd
url = "/content/train.csv"                  # Load dataset from Local system. 
df = pd.read_csv(url)
df.head()                                     # Display first 5 rows
print("Dataset Shape:", df.shape)            # Check shape
print("\nDataset Info:")

What .info() Shows
Column names
Data types (int, float, object)
Missing values (non-null counts)
Memory usage
df.info()                                     # Check dataset info

df.describe()                                  # Generate descriptive statistics
The descriptive statistics show that the average house sale price is around 1.8 lakh, with a large standard deviation, indicating high variability in house prices. The dataset contains 1460 records and 81 features. The lot area and sale price show wide ranges, suggesting the presence of potential outliers, making this dataset suitable for outlier detection.

The dataset contains several columns with high percentages of missing values. Features such as PoolQC, MiscFeature, Alley, and Fence have more than 80% missing values, indicating these attributes are mostly unavailable and may be removed or carefully imputed. Columns like LotFrontage and garage-related features show moderate missing values, which require appropriate data cleaning techniques before analysis.

# Calculate missing value percentage for each column
missing_percent = (df.isnull().sum() / len(df)) * 100

# Convert to DataFrame and sort
missing_df = pd.DataFrame({
    'Missing Values %': missing_percent
}).sort_values(by='Missing Values %', ascending=False)
missing_df.head(20)

Histograms:
Show distribution of values (normal, skewed, etc.)
Helps identify skewed features (like SalePrice, LotArea)

Boxplots:
Show median, quartiles, and outliers
Outliers appear as dots outside the whiskers
Features like SalePrice, LotArea, GrLivArea often have extreme outliers
Using .dropna() ensures missing values don‚Äôt break the plots.

import matplotlib.pyplot as plt
import math

# Select numerical columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
n_cols = 4  # number of subplots per row
n_rows = math.ceil(len(num_cols) / n_cols)  # compute rows needed

# Histograms
plt.figure(figsize=(20, 5*n_rows))                               #plt.figure(figsize=(20, 5*n_rows)) ‚Üí adjusts figure height automatically.
for i, col in enumerate(num_cols):
    plt.subplot(n_rows, n_cols, i+1)
    plt.hist(df[col].dropna(), bins=30, color='skyblue', edgecolor='black')
    plt.title(f'Histogram of {col}')
plt.tight_layout()
plt.show()

# Boxplots
plt.figure(figsize=(20, 5*n_rows))                #plt.figure(figsize=(20, 5*n_rows)) ‚Üí adjusts figure height automatically.
for i, col in enumerate(num_cols):
    plt.subplot(n_rows, n_cols, i+1)
    plt.boxplot(df[col].dropna())
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()


Histograms reveal that features such as SalePrice and LotArea are right-skewed, meaning there are a few very large values. Boxplots confirm this and highlight extreme outliers, which can be further handled using IQR or Z-score methods. Features like OverallQual or YearBuilt are more evenly distributed, showing less extreme variation.

# Select numerical columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Dictionary to store outlier indices
outliers_dict = {}

# Detect outliers for each numerical column
for col in num_cols:
    Q1 = df[col].quantile(0.25)   # 25th percentile
    Q3 = df[col].quantile(0.75)   # 75th percentile
    IQR = Q3 - Q1                  # Interquartile range
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Get indices of outliers
    outlier_indices = df[(df[col] < lower_bound) | (df[col] > upper_bound)].index
    outliers_dict[col] = outlier_indices

# Example: Number of outliers per column
outlier_counts = {col: len(indices) for col, indices in outliers_dict.items()}
outlier_counts


Computes Q1, Q3, and IQR for each numerical column.

Defines outlier bounds:
Lower Bound= ùëÑ1 ‚àí 1.5 √ó ùêºùëÑùëÖ,
Upper Bound = ùëÑ3 + 1.5 √ó ùêºùëÑùëÖ
Lower Bound=Q1‚àí1.5√óIQR,Upper Bound=Q3+1.5√óIQR

Finds all rows with values outside these bounds.
Stores indices of outliers in outliers_dict for each column.
Shows number of outliers per column with outlier_counts.

# Select numerical columns
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Initialize a column 'Outlier_Flag' with 0 (no outlier)
df['Outlier_Flag'] = 0

# Detect outliers and flag them
for col in num_cols:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Mark as 1 if outlier
    df.loc[(df[col] < lower_bound) | (df[col] > upper_bound), 'Outlier_Flag'] = 1

# Check first 10 rows to verify
df[['Outlier_Flag'] + list(num_cols[:5])].head(10)
What This Does

Adds a new column: Outlier_Flag
0 ‚Üí Row has no outliers
1 ‚Üí Row has at least one outlier
Checks all numerical columns for outliers using IQR.
Can be used for filtering or analysis, e.g.:

# Number of rows with outliers
print("Rows with outliers:", df['Outlier_Flag'].sum())

# Filter only outliers
df_outliers = df[df['Outlier_Flag'] == 1]

# Filter only clean rows
df_clean = df[df['Outlier_Flag'] == 0]

Why Handle Outliers
Outliers can distort mean, std, and correlation, affecting models like linear regression.
Some features, like SalePrice or LotArea, have extreme values that may not represent typical homes.
But not all outliers are errors: e.g., very expensive houses are valid.

Rule of thumb:
Remove rows ‚Üí if the value is clearly erroneous or extremely rare
Cap values ‚Üí if you want to keep all rows but reduce impact.

Handling Outliers Using IQR
We‚Äôll use the IQR method, which we already applied for detection.

# Copy dataset
df_removed = df.copy()

# Remove rows where 'Outlier_Flag' == 1
df_removed = df_removed[df_removed['Outlier_Flag'] == 0]

print("Shape before removing outliers:", df.shape)
print("Shape after removing outliers:", df_removed.shape)
Reasoning:
Removes rows that have at least one extreme value.
Useful if dataset is large enough and we want clean statistical analysis

df_capped = df.copy()

for col in num_cols:
    Q1 = df_capped[col].quantile(0.25)
    Q3 = df_capped[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Cap values at bounds
    df_capped[col] = df_capped[col].clip(lower=lower_bound, upper=upper_bound)

# Verify new min/max for a sample column
print(df_capped['SalePrice'].describe())

Keeps all rows, but reduces extreme impact.
Maintains sample size, important for ML models or correlation analysis.
Useful if outliers are valid extreme values, not errors.

import seaborn as sns
import matplotlib.pyplot as plt

# Select numerical columns only
num_cols = df.select_dtypes(include=['int64', 'float64']).columns

# Compute correlation matrix
corr_matrix = df[num_cols].corr()

# Plot heatmap
plt.figure(figsize=(15, 12))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title("Correlation Matrix of Numerical Features", fontsize=16)
plt.show()

# Top correlations with SalePrice
top_corr = corr_matrix['SalePrice'].sort_values(ascending=False)
print("Top correlations with SalePrice:\n", top_corr.head(10))

Features like OverallQual, GrLivArea, and GarageCars are the most important predictors of house price. Other features like LotArea and YearRemodAdd show moderate correlation. This information is useful for feature selection in regression models.

# Suppose df_capped or df_removed is your cleaned dataset
# We'll use df_capped (after capping outliers) for this example

# Export to CSV
df_capped.to_csv("house_prices_cleaned.csv", index=False)
print("Cleaned dataset exported successfully!")

# Load exported file to check
df_check = pd.read_csv("house_prices_cleaned.csv")
df_check.head()
df_check.shape

